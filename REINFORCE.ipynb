{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "REINFORCE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTvGN1u662QS"
      },
      "source": [
        "Required Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuIh5BbS61aV"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import gym\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmXdcnj7-yW2"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"You are using device: %s\" % device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orMeoVRd68Kz"
      },
      "source": [
        "REINFORCE neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqqQm4mR6wDp"
      },
      "source": [
        "def _weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "class Reinforce(torch.nn.Module):\n",
        "    def __init__(self, H):\n",
        "        \"\"\"\n",
        "        Note that we default the input size to be 4 (because of the 4 observations)\n",
        "        and the output size to be 2 (because of the two possible actions, left and right).\n",
        "        \"\"\"\n",
        "        super(Reinforce, self).__init__()\n",
        "        self.reinforce = nn.Sequential(\n",
        "                                       nn.Linear(8, H),\n",
        "                                       nn.ReLU(),\n",
        "                                       nn.Linear(H, 4),\n",
        "                                       nn.Softmax(dim=1)\n",
        "\n",
        "        )\n",
        "        self.apply(_weights_init)\n",
        "        self.log_probs = []\n",
        "        self.rewards = []\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Not much to add here.\n",
        "        \"\"\"\n",
        "        out = self.reinforce(x)\n",
        "        return out\n",
        "\n",
        "    def act(self,state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        probs = self.forward(state)\n",
        "        m = Categorical(probs=probs)\n",
        "        selected_action = m.sample()\n",
        "        self.log_probs.append(m.log_prob(selected_action))\n",
        "        return selected_action.item()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-lmUHNm7GKP"
      },
      "source": [
        "def reinforce(num_episodes, gamma):\n",
        "    reward_list_plot = []\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        score = 0\n",
        "        while not done:\n",
        "            #env.render()\n",
        "            action = agent.act(state)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            agent.rewards.append(reward)\n",
        "            score += reward\n",
        "\n",
        "            if done:\n",
        "                if episode % 100 == 0:\n",
        "                    print(f\"Currently on episode {episode}\")\n",
        "                reward_list_plot.append(score)\n",
        "                if episode == (num_episodes - 1):\n",
        "                    numbers_series = pd.Series(reward_list_plot)\n",
        "                    windows = numbers_series.rolling(25)\n",
        "                    moving_averages = windows.mean()\n",
        "                    moving_averages_list = moving_averages.tolist()\n",
        "\n",
        "                    plt.plot(reward_list_plot)\n",
        "                    plt.plot(moving_averages_list)\n",
        "                    plt.legend([\"Raw Score Per Episode \", \"Moving Average Score\"], loc='lower right')\n",
        "                    plt.xlabel('Episode (REINFORCE)')\n",
        "                    plt.ylabel('Episode Score')\n",
        "                    plt.show()\n",
        "                break\n",
        "        finish_episode(gamma)\n",
        "    env.close()\n",
        "\n",
        "def finish_episode(gamma):\n",
        "    policy_loss = []\n",
        "    eps = np.finfo(np.float32).eps.item()\n",
        "    discounts = [gamma ** i for i in range(len(agent.rewards))]\n",
        "    rewards = torch.tensor(agent.rewards)\n",
        "    discounts = torch.tensor(discounts)\n",
        "    rewards = discounts * rewards\n",
        "    rewards = (rewards - rewards.mean()) / (rewards.std() + eps)\n",
        "    for log_prob, reward in zip(agent.log_probs, rewards):\n",
        "        policy_loss.append(reward * -log_prob)\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss = torch.cat(policy_loss).sum()\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "    del agent.rewards[:]\n",
        "    del agent.log_probs[:]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMfYff6k7Ju7"
      },
      "source": [
        "Running the agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgZX-OFL80Sc"
      },
      "source": [
        "!!pip3 install box2d-py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km9jkRmK7JQa"
      },
      "source": [
        "env = gym.make('LunarLander-v2')\n",
        "\n",
        "agent = Reinforce(256).to(device)\n",
        "optimizer = torch.optim.Adam(agent.parameters(), lr=1e-2)\n",
        "reinforce(300,0.99)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
